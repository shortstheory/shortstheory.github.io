Over the past few months or so, I've had to work on getting pretrained deep learning models downloaded off GitHub to run for some robotics projects. In doing so, I've had to work with getting inference to run on an NVIDIA GPU and in this post, I want to highlight my experiences.

The good

With the growth in machine learning applications in the last ten years, it looks like there's never been a better time for NVIDIA. The NVIDIA CUDA libraries has turned their gaming GPUs into general purpose computing monsters overnight. Vector and matrix operations are crucial to several ML algorithms, and they run several times faster on the GPU than they do on the fastest CPUs. To solidify their niche in this market, the 10+/20+ series of NVIDIA GPUs come with dedicated hardware blocks (such as the Deep Learning Accelerator (DLA) and FP16 optimizations) for faster inference performance.

NVIDIA has developed several software libraries to run optimized ML models on their GPUs. The alphabet soup of ML frameworks and their file extensions took me some time to decipher at first, but the actual workflow of model optimization is surprisingly simple. Once you have a trained your PyTorch or Tensorflow model, you can convert the saved model or frozen checkpoint to the ONNX model exchange format. ONNX lets you convert your ML model from one format to another, and in the case of NVIDIA GPUs, the format of choice is a TensorRT engine file. TensorRT engine files are bespoke, optimized inference models for a given configuration of CUDA, NVIDIA ML libraries, GPU, and decimal precision. Hence, they can only be generated *on* the GPU you want to use for deployment. 

When the TensorRT conversion works as it's supposed to, you can expect to see a significant 2-3x performance boost over running the Tensorflow/PyTorch model. INT8 models might have even better performance, but this requires a calibration dataset for constructing the TensorRT engine. TensorRT is great for deploying models on the NVIDIA Xavier NX board, since you can also construct TensorRT engines which run on the power efficient DLAs instead of the GPU. 

Room for improvement

For all of NVIDIA's good work in developing highly performant ML libraries for their GPUs, the user experience of using ML development software leaves some to be desired. It's *imperative* to double check all the version numbers of the libraries you need when doing a fresh installation. For instance, a particular version of `tensorflow-gpu` only works with a specific point release of CUDA which in turn depends on using the right GPU driver version. All of these need to be installed independently, so there are many sources of error which can result in a broken installation. Installing the wrong version of TensorRT and cuDNN can also result in dependency hell. I am curious to know why all these libraries are so interdependent on specific point releases and why the support for backward compatibility of these libraries is so minimal. 