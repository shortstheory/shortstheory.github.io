<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>~Weblog~ - Ardupilot</title><link href="http://arnavdhamija.com/" rel="alternate"></link><link href="http://arnavdhamija.com/feeds/tag_ardupilot.atom.xml" rel="self"></link><id>http://arnavdhamija.com/</id><updated>2018-07-22T22:30:00+05:30</updated><entry><title>GSoC 2018 - Batteries Included!</title><link href="http://arnavdhamija.com/blog/ardupilot-gsoc-update.html" rel="alternate"></link><published>2018-07-22T22:30:00+05:30</published><updated>2018-07-22T22:30:00+05:30</updated><author><name>Arnav Dhamija</name></author><id>tag:arnavdhamija.com,2018-07-22:/blog/ardupilot-gsoc-update.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="http://arnavdhamija.com/images/qcopter-stock.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Much time has passed and much code has been written since my last update. Adaptive Streaming (a better name TBD) for Ardupilot is nearly complete and brings a whole slew of features useful for streaming video from cameras on robots to laptops, phones, and tablets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automatic quality selection based on â€¦&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="http://arnavdhamija.com/images/qcopter-stock.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Much time has passed and much code has been written since my last update. Adaptive Streaming (a better name TBD) for Ardupilot is nearly complete and brings a whole slew of features useful for streaming video from cameras on robots to laptops, phones, and tablets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automatic quality selection based on bandwidth and packet loss estimates&lt;/li&gt;
&lt;li&gt;Options to record the live-streamed video feed to the companion computer (experimental!)&lt;/li&gt;
&lt;li&gt;Fine tuned control over resolution and framerates&lt;/li&gt;
&lt;li&gt;Multiple camera support over RTSP&lt;/li&gt;
&lt;li&gt;Hardware-accelerated H.264 encoding for supported cameras and GPUs&lt;/li&gt;
&lt;li&gt;Camera settings configurable through the APWeb GUI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Phew!&lt;/p&gt;
&lt;p&gt;The configuration required to get everything working is minimal once the required dependencies have been installed. This is in no small part possible thanks to the GStreamer API which took care of several low level complexities of live streaming video over the air.&lt;/p&gt;
&lt;p&gt;Streaming video from aerial robots is probably the most difficult use case of Adaptive Streaming as the WiFi link is very flaky at these high speeds and distances. I've optimised the project around my testing with video streaming from quadcopters so the benefits are passed on to streaming from other robots as well.&lt;/p&gt;
&lt;h2&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;I've used a simplification of TCP's &lt;a href="https://en.wikipedia.org/wiki/TCP_congestion_control"&gt;congestion control&lt;/a&gt; algorithm for Adaptive Streaming. I had looked at other interesting approaches including estimating receiver &lt;a href="https://www.researchgate.net/publication/280738389_An_Analysis_of_TCP-Tolerant_Real-Time_Multimedia_Distribution_in_Heterogeneous_Networks?_sg=pcxT2q90osdkY06gupLQqwssRN0DZrsL3zP2oyqKVIjTML5RhEIWWX5S3-N4KbDRVqHbTc3i2VNzBBpVuQ72t9iSWyT10_8i6w"&gt;buffer occupancy&lt;/a&gt;, but using this approach didn't yield significantly better results. TCP's congestion control algorithm avoids packet loss by mandating ACKs for each successfully delivered packet and steadily increasing sender bandwidth till it reaches a dynamically threshold.&lt;/p&gt;
&lt;p&gt;A crucial difference for Adaptive Streaming is that 1) we stream over UDP for lower overhead (so no automatic TCP ACKs here!) 2) H.264 live video streaming is fairly loss tolerant so it's okay to lose some packets instead of re-transmitting them. &lt;/p&gt;
&lt;p&gt;Video packets are streamed over dedicated RTP packets and Quality of Service (QoS) reports are sent over RTCP packets. These QoS reports give us all sorts of useful information, but we're mostly interested in seeing the number of packets loss between RTCP transmissions.&lt;/p&gt;
&lt;p&gt;On receiving a RTCP packet indicating any packet loss, we immediately shift to a Congested State (better name pending) which significantly reduces the rate at which video streaming bandwidth is increased on receiving a lossless RTCP packet. The encoding H.264 encoding bitrate is limited to no higher than 1000kbps in this state. &lt;/p&gt;
&lt;p&gt;Once we've received five lossless RTCP packets, we shift to a Steady State which can encode upto 6000kbps. In this state we also increase the encoding bitrate at a faster rate than we do in the Congested State. A nifty part of dynamically changing H.264 bitrates is that we can also seamlessly switch the streamed resolution according to the available bandwidth, just like YouTube does with DASH!&lt;/p&gt;
&lt;p&gt;This algorithm is fairly simple and wasn't too difficult to implement once I had figured out all the GStreamer plumbing for extracting packets from buffers. With more testing, I would like to add long-term bitrate adaptations for the bitrate selection algorithm.&lt;/p&gt;
&lt;h2&gt;H.264 Encoding&lt;/h2&gt;
&lt;p&gt;This is where we get into the complicated and wonderful world of video compression algorithms.&lt;/p&gt;
&lt;p&gt;Compression algorithms are used in all kinds of media, such as JPEG for still images and MP3 for audio. H.264 is one of several compression algorithms available for video. H.264 takes advantage of the fact that a lot of the information in video between frames is redundant. so instead of saving 30 frames for 1 second of 30fps video, it saves one entire frame (known as the Key Frame or I-Frame) of video and computes and stores only the differences in frames with respect to the keyframe for the subsequent 29 frames. H.264 also applies some logic to &lt;em&gt;predict&lt;/em&gt; future frames to further reduce the file size. &lt;/p&gt;
&lt;p&gt;This is by no means close to a complete explanation of how H.264 works, for further reading I suggest checking out Sid Bala's &lt;a href="https://sidbala.com/h-264-is-magic/"&gt;explanation&lt;/a&gt; on the topic.&lt;/p&gt;
&lt;p&gt;The legendary Tom Scott also has a fun &lt;a href="https://www.youtube.com/watch?v=r6Rp-uo6HmI"&gt;video explaining how H.264 is adversely affected by snow and confetti&lt;/a&gt;!&lt;/p&gt;
&lt;div class="youtube youtube-16x9"&gt;
&lt;iframe src="https://www.youtube.com/embed/r6Rp-uo6HmI" allowfullscreen seamless frameBorder="0"&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The frequency of capturing keyframes can be set by changing the encoder parameters. In the context of live video streaming over unstable links such as WiFi, this is very important as packet loss can cause keyframes to be dropped. Dropped keyframes severely impact the quality of the video until a new keyframe is received. This is because all the frames transmitted after the keyframe only store the differences with respect to the keyframe and do not actually store a full picture on their own.&lt;/p&gt;
&lt;p&gt;Increasing the keyframe interval means we send a large, full frame less frequently, but also means we would suffer from terrible video quality for an extended period of time on losing a keyframe. On the other hand, shorter keyframe intervals can lead to a wastage of bandwidth.&lt;/p&gt;
&lt;p&gt;I found that a keyframe interval of every 10 frames worked much better than the default interval of 60 frames without impacting bandwidth usage too significantly.&lt;/p&gt;
&lt;p&gt;Lastly, H.264 video encoding is a very computationally expensive algorithm. Software-based implementations of H.264 such as &lt;code&gt;x264enc&lt;/code&gt; are well supported with several configurable parameters but have prohibitively high CPU requirements, making it all but impossible to livestream H.264 encoded video from low power embedded systems. Fortunately, the Raspberry Pi's Broadcom BCM2837 SoC has a dedicated H.264 hardware encoder pipeline for the Raspberry Pi camera which drastically reduces the CPU load in high definition H.264 encoding. Some webcams such as the Logitech C920 and higher have onboard H.264 hardware encoding thanks to special ASIC's dedicated for this purpose.&lt;/p&gt;
&lt;p&gt;Adaptive Streaming probes for the type of encoding supported by the webcam and whether it has the IOCTL's required for changing the encoding parameters on-the-fly.&lt;/p&gt;
&lt;p&gt;H.264 has been superseded by the more efficient H.265 encoding algorithm, but the CPU requirements for H.265 are even higher and it doesn't enjoy the same hardware support as H.264 does for the time being.&lt;/p&gt;
&lt;h2&gt;GUI&lt;/h2&gt;
&lt;p&gt;The project is soon-to-be integrated with the APWeb project for configuring companion computers. Adaptive Streaming works by creating an RTSP Streaming server running as a daemon process. The APWeb process connects to this daemon service over a local socket to populate the list of cameras, RTSP mount points, and available resolutions of each camera.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://arnavdhamija.com/images/apweb-screenshot.png"&gt;&lt;/p&gt;
&lt;p&gt;The GUI is open for improvement and I would love feedback on how to make it easier to use!&lt;/p&gt;
&lt;p&gt;Once the RTSP mount points are generated, one can livestream the video feed by entering in the RTSP URL of the camera into VLC. This works on all devices supporting VLC. However, VLC does add two seconds of latency to the livestream for reducing the jitter. I wasn't able to find a way to configure this in VLC, so an alternative way to get a lower latency stream is by using the following &lt;code&gt;gst-launch&lt;/code&gt; command in a terminal:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;gst-launch-1.0 playbin uri=&amp;lt;RTSP Mount Point&amp;gt; latency=100&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In the scope of the GSoC timeline, I'm looking to wind down the project by working on documentation, testing, and reducing the cruft from the codebase. I'm looking forward to integrating this with companion repository soon!&lt;/p&gt;
&lt;h2&gt;Links to the code&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/shortstheory/adaptive-streaming"&gt;https://github.com/shortstheory/adaptive-streaming&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/shortstheory/APWeb"&gt;https://github.com/shortstheory/APWeb&lt;/a&gt;&lt;/p&gt;</content><category term="Ardupilot"></category><category term="GSoC"></category><category term="Programming"></category></entry><entry><title>GSoC 2018 - New Beginnings</title><link href="http://arnavdhamija.com/blog/ardupilot-gsoc-intro.html" rel="alternate"></link><published>2018-06-05T18:30:00+05:30</published><updated>2018-06-05T18:30:00+05:30</updated><author><name>Arnav Dhamija</name></author><id>tag:arnavdhamija.com,2018-06-05:/blog/ardupilot-gsoc-intro.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="http://arnavdhamija.com/images/ardupilot_logo.jpg"&gt;&lt;/p&gt;
&lt;p&gt;I'm really excited to say that I'll be working with &lt;a href="ardupilot.org"&gt;Ardupilot&lt;/a&gt; for the better part of the next two months! Although this is the second time I'm making a foray into Open Source Development, the project at hand this time is quite different from what I had worked on in â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="http://arnavdhamija.com/images/ardupilot_logo.jpg"&gt;&lt;/p&gt;
&lt;p&gt;I'm really excited to say that I'll be working with &lt;a href="ardupilot.org"&gt;Ardupilot&lt;/a&gt; for the better part of the next two months! Although this is the second time I'm making a foray into Open Source Development, the project at hand this time is quite different from what I had worked on in my first GSoC project.&lt;/p&gt;
&lt;p&gt;Ardupilot is an open-source autopilot software for several types of semi-autonomous robotic vehicles including multicopters, fixed-wing aircraft, and even marine vehicles such as boats and submarines. As the name suggests, Ardupilot was formerly based on the Arduino platform with the APM2.x flight controllers which boasted an ATMega2560 processor. Since then, Ardupilot has moved on to officially supporting much more advanced boards with significantly better processors and more robust hardware stacks. That said, these flight controllers contain application specific embedded hardware which is unsuitable for performing intensive tasks such as real-time object detection or video processing.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://arnavdhamija.com/images/apsync-configurator.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;CC Setup with a Flight Computer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ardupilot.org/dev/docs/apsync-intro.html"&gt;APSync&lt;/a&gt; is a recent Ardupilot project which aims to ameliorate the limited processing capability of the flight controllers by augmenting them with so-called companion computers (CCs). As of writing, APSync officially supports the Raspberry Pi 3B(+) and the NVidia Jetson line of embedded systems. One of the more popular use cases for APSync is to enable out-of-the-box live video streaming from a vehicle to a laptop. This works by using the CC's onboard WiFi chip as a WiFi hotspot to stream the video using GStreamer. However, the current implementation has some shortcomings which are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Only one video output can be unicasted from the vehicle&lt;/li&gt;
&lt;li&gt;The livestreamed video progressively deteriorates as the WiFi link between the laptop and the CC becomes weaker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is where my GSoC project comes in. My project is to tackle the above issues to provide a good streaming experience from an Ardupilot vehicle. The former problem entails rewriting the video streaming code to allow for sending multiple video streams at the same time. The latter is quite a bit more interesting and it deals with several computer networks and hardware related engineering issues to solve. "Solve" is a subjective term here as there isn't any way to significantly boost the WiFi range from the CC's WiFi hotspot without some messy hardware modifications.&lt;/p&gt;
&lt;p&gt;What can be done is to degrade the video quality as gracefully as possible. It's much better to have a smooth video stream of low quality than to have a high quality video stream laden with jitter and latency. At the same time, targeting to only stream low quality video when the WiFi link and the processor of the CC allows for better quality is inefficient. To "solve" this, we would need some kind of dynamically adaptive streaming mechanism which can change the quality of the video streamed according to the strength of the WiFi connection.&lt;/p&gt;
&lt;p&gt;My first thought was to use something along the lines of Youtube's &lt;a href="https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP"&gt;DASH&lt;/a&gt; (Dynamically Adaptive Streaming over HTTP) protocol which automatically scales the video quality according to the available bandwidth. However, DASH works in a fundamentally different way from what is required for adaptive livestreaming. DASH relies on having the same video pre-encoded in several different resolutions and bitrates. The server estimates the bandwidth of its connection to the client. On doing so, the server chooses one of the pre-encoded video chunks to send to the client. Typically, the server tries to guess which video chunk can deliver the best possible quality without buffering.&lt;/p&gt;
&lt;p&gt;Youtube's powerful servers have no trouble encoding a video several times, but this approach is far too intensive to be carried out on a rather anemic Raspberry Pi. Furthermore, DASH relies on QoS (short for Quality of Service which includes parameters like bitrate, jitter, packet loss, etc) reports using TCP ACK messages. This causes more issues as we need to stream the video down using RTP over UDP instead of TCP. The main draw of UDP for livestreaming is that performs  better than TCP does on low bandwidth connections due to its smaller overhead. Unlike TCP which places guarantees on message delivery through ACKs, UDP is purely best effort and has no concept of ACKs at the transport layer. This means we would need some kind of ACK mechanism at the application layer to measure the QoS.&lt;/p&gt;
&lt;p&gt;Enter &lt;a href="https://tools.ietf.org/html/rfc3550"&gt;RTCP&lt;/a&gt;. This is the official sibling protocol to RTP which among other things, reports packet loss, cumulative sequence number received, and jitter. In other words - it's everything but the kitchen sink for QoS reports for multimedia over UDP! What's more, GStreamer natively integrates RTCP report handling. This is the approach I'll be using for getting estimated bandwidth reports from each receiver.&lt;/p&gt;
&lt;p&gt;I'll be sharing my experiences with the H.264 video encoders and hardware in my next post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other links&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1) My GSoC Proposal for &lt;a href="https://docs.google.com/document/d/17iZgdBqVHGa-ny3XQ73sAKmYxeUcWsS3eeeKzBS8F4s/edit?usp=sharing"&gt;Ardupilot&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2) Previous &lt;a href="http://arnavdhamija.com/blog/tag/gsoc.html"&gt;GSoC Posts&lt;/a&gt;&lt;/p&gt;</content><category term="Ardupilot"></category><category term="GSoC"></category><category term="Programming"></category></entry></feed>