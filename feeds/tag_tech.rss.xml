<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>A Singularity</title><link>http://arnavdhamija.com/</link><description></description><atom:link href="http://arnavdhamija.com/feeds/tag_tech.rss.xml" rel="self"></atom:link><lastBuildDate>Fri, 09 Jun 2017 15:30:00 +0530</lastBuildDate><item><title>The Y2038 Glitch</title><link>http://arnavdhamija.com/blog/y2038.html</link><description>&lt;p&gt;&lt;em&gt;I had written this article a bit more than a year ago for a college magazine. This topic has been done to death in a Q/A format, so if you're looking for something new about Y2038, you probably won't find it here&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Time has always been a finicky thing to deal with. Our perception of time without any stimulus is limited to less than an hour. We would be severely crippled without our abundance of electronic devices synchronized by internet atomic clocks. Unfortunately, these electronic devices on which we're so reliant on are heading towards a time based computer disaster on the same scale as the Y2K bug of this millennium - known as the Y2038 problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do computers measure time and what is the Y2038 bug?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All UNIX derivative systems (including your iPhone or Android smartphone) measure time by counting the number of milliseconds that have elapsed from the 00:00 UTC, 1 January 1970. This day and time is known as the UNIX epoch. While this method of measuring time has served us well for a number of years, it faces a severe limitation - the &lt;code&gt;time_t&lt;/code&gt; int variable (an int variable is a part of a program used to hold integral values) in UNIX used to store the number of milliseconds from the epoch is only a 32-bit data type on older 32-bit computers. This means the &lt;code&gt;time_t&lt;/code&gt; variable can only store up to 2&lt;sup&gt;31&lt;/sup&gt; - 1 milliseconds before the counter overflows.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: 32-bit systems can hold a 64-bit int by splitting it into two words each of length 32-bits. However, the default int size on 32-bit systems is only 32-bits and a 64-bit int has to be coded separately.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So how long is it between before this overflow?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Exactly 2&lt;sup&gt;31&lt;/sup&gt; - 1 milliseconds after 00:00 UTC, 1 January 1970, which is 03:14:07 UTC, 19 January 2038.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So what will happen?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The overflow in the integer value will cause &lt;code&gt;time_t&lt;/code&gt; to reset to - (2&lt;sup&gt;31&lt;/sup&gt;) milliseconds. This date is 20:45 UTC, December 1901. This is actually a much more serious situation than just your computer showing a funny date. This has disastrous consequences for systems with 32-bit CPUs as BIOS software, file systems, databases, network security certificates and a wide variety embedded hardware will fail to work. With huge critical machinery such as old electrical power stations controlled by a 32-bit computer, an unmitigated catastrophe is unavoidable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Oh no! Does that mean my laptop and smartphone will stop working as well?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Depends. The 32-bit &lt;code&gt;time_t&lt;/code&gt; variable has been deprecated and replaced by a 64-bit &lt;code&gt;time_t&lt;/code&gt; variable which will tide us over for the next 2 billion years. 64-bit CPUs are the standard nowadays and most computers running a 64-bit OS on a 64-bit CPU will not face any such consequences from the Y2038 bug as they use the 64-bit length &lt;code&gt;time_t&lt;/code&gt; variable. However, smartphones have only recently shifted to a 64-bit process and 32-bit smartphones and computers will be affected if they are not coded for the 64-bit int length. That is of course, only if you are still using it after 23 years from today :P&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But how did this even happen? Why couldn't we just use a 64-bit &lt;code&gt;time_t&lt;/code&gt; variable in the first place? It's only 4 bytes bigger!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Short answer: Legacy.&lt;/p&gt;
&lt;p&gt;Long answer: The UNIX kernel was created in AT&amp;amp;T and Bell labs in the 1970s. There were many competent programmers working on the project such as Dennis Ritchie of C fame. Initially, Bell engineers used a different method for calculating time, but they found that the counter would only work for 2.5 years. As UNIX was planned to be a long project, they changed the time counter to a 4-byte integer. This of course was also limited to a very finite amount of time. However in the 1970s, the consensus was that computers weren't going to stick around for that much longer and a 60-odd year window was "good enough".&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Couldn't they just have just used a 64-byte int anyway?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Not really. Computer resources were scarce in the 1970s and 8-bytes of memory to store &lt;code&gt;time_t&lt;/code&gt; was more memory than what engineers were willing to give.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wait a second! Why doesn't &lt;code&gt;time_t&lt;/code&gt; use the unsigned int? Surely UNIX wasn't programmed expecting us to go back in time!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;time_t&lt;/code&gt; variable uses the signed int to account for dates before the UNIX epoch. Dates before 1 January, 1970 are represented in a negative number of milliseconds which is why an overflow would take the computer's date to 1901.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But this is 2017! Can't we just upgrade the &lt;code&gt;time_t&lt;/code&gt; variable to 64-bits?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Yes we can. In all new software and 64-bit operating systems, the &lt;code&gt;time_t&lt;/code&gt; variable has been changed to 8-bytes. The problem is that all legacy software compiled with other older compilers may be incompatible for recompiling with a newer compiler for an 8-byte int value. The problem doesn't just stop there. Many embedded systems and microcontrollers (including the popular Arduino ATMega based platform) use 16-bit CPUs which simply cannot support an 8-byte integer in a 2-byte word length. Some hardware applications may use proprietary firmware which won't receive updates.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can we fix the Y2038 problem?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There isn't any one-solution-fits-all approach for this since it affects several computer architectures, hardware, and software. While it might be easier to just upgrade all old computers with new 64-bit capable ones, there are still many areas where old 32-bit code is still prevalent. Furthermore, the cost of upgrading all the computer hardware is just too exorbitant to be covered.&lt;/p&gt;
&lt;p&gt;If we look at the lessons learned from the past, the Y2K bug was rectified because of the media hype pressuring businesses to update their software to accommodate 4-digit years. Although the Y2038 bug is much more critical and harder to understand than the Y2K bug, there is hope that the same pressure will bring change.&lt;/p&gt;
&lt;p&gt;But for now, only time will tell.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://imgs.xkcd.com/comics/2038.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Credits: &lt;a href="https://xkcd.com/"&gt;xkcd&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Arnav Dhamija</dc:creator><pubDate>Fri, 09 Jun 2017 15:30:00 +0530</pubDate><guid>tag:arnavdhamija.com,2017-06-09:blog/y2038.html</guid><category>Tech</category><category>Misc</category></item></channel></rss>